{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/09/ed/b59a2edde05b7f5755ea68648487c150c7c742361e9c8733c6d4ca005020/gensim-3.8.1-cp37-cp37m-win_amd64.whl (24.2MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Collecting boto>=2.32 (from smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/ef/10bfa08a204a59ccef56a922b19c1648ce9cb61b6d127c607314a12d498d/boto3-1.11.13-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.15.0,>=1.14.13 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/ef/1860283d82ab7eaa85a911fc038ab3a9646efa0b1393a9b8a2f8c0905f96/botocore-1.14.13-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in c:\\users\\bellachenhui\\appdata\\roaming\\python\\python37\\site-packages (from botocore<1.15.0,>=1.14.13->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.15.0,>=1.14.13->boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-1.9.0-cp37-none-any.whl size=73092 sha256=0bc19b270317adeeaacf56836bcb0b9b6c4a1e8d4712fc9a34bc8520f609e6cc\n",
      "  Stored in directory: C:\\Users\\BellaChenhui\\AppData\\Local\\pip\\Cache\\wheels\\ab\\10\\93\\5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.11.13 botocore-1.14.13 docutils-0.15.2 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.3.3 smart-open-1.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[ 1.7001590e-03 -4.7963497e-04 -7.7488396e-04  1.8057234e-03\n",
      " -4.9905973e-03 -4.1614966e-03 -3.7707586e-03 -4.9362611e-03\n",
      "  2.3134276e-03 -3.7801315e-03  6.3007150e-04 -1.9581993e-03\n",
      "  7.5105968e-04  3.8503055e-03  2.8657953e-03  3.4395345e-03\n",
      "  4.5661642e-03  4.9267407e-03 -1.1919237e-03 -1.5744556e-03\n",
      " -1.3669820e-03  4.7579794e-03  6.9342984e-04  7.5123005e-04\n",
      "  4.9528619e-03  2.6036389e-03 -1.8600691e-03  2.4773874e-03\n",
      " -3.9517358e-03 -2.5517058e-03 -4.2403885e-03  2.9844907e-03\n",
      " -1.6633219e-03  9.7145932e-04  4.8856246e-03  3.7449191e-03\n",
      "  6.8970007e-04  4.3471446e-03 -6.4323144e-04 -2.3390858e-03\n",
      " -8.4266230e-04 -6.7311725e-05  9.6790004e-04 -2.3365428e-03\n",
      " -3.4990117e-03  4.8096464e-03 -2.0884996e-04 -9.4785623e-04\n",
      " -2.0506198e-03 -1.1034046e-03  1.1443420e-03  2.3057947e-03\n",
      "  2.7607444e-03  4.3860213e-03  3.7471766e-03  4.0892269e-03\n",
      "  2.6281597e-03 -3.4143275e-03  9.3469908e-04  4.2201201e-03\n",
      " -1.1562028e-04 -2.9986757e-03 -2.3593213e-03  4.7053336e-03\n",
      "  4.4957702e-03  3.9789607e-03  2.4175115e-03 -2.5739563e-03\n",
      " -3.6581783e-04  2.2502866e-04 -2.2058110e-03  1.8966984e-03\n",
      " -3.1687387e-03 -2.5781168e-04  2.2812551e-03 -4.8714899e-03\n",
      "  4.9005233e-04  2.7106036e-03 -4.8031617e-04 -4.7676107e-03\n",
      " -1.2003660e-03  2.7340933e-04 -3.4116360e-03 -3.4328888e-03\n",
      "  2.7515269e-03  4.1724141e-03  3.8914033e-03  2.8758687e-03\n",
      " -3.8790924e-03  4.8410539e-03 -1.9330642e-03 -4.8766769e-03\n",
      " -3.1904166e-03  2.5400692e-03  2.5586474e-03 -4.9588485e-03\n",
      " -2.1764191e-03  3.0565809e-03  3.7061819e-03 -1.5474615e-03]\n",
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BellaChenhui\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BellaChenhui\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['sentence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip\n",
      "Requirement already satisfied: six in c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449913 sha256=dde8e583cb28ac7069f16551bcc29217d08f3621316165a7e3b97645fe392baa\n",
      "  Stored in directory: C:\\Users\\BellaChenhui\\AppData\\Local\\pip\\Cache\\wheels\\96\\86\\f6\\68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.doc2vec in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.doc2vec\n",
      "\n",
      "DESCRIPTION\n",
      "    Learn paragraph and document embeddings via the distributed memory and distributed bag of words models from\n",
      "    `Quoc Le and Tomas Mikolov: \"Distributed Representations of Sentences and Documents\"\n",
      "    <http://arxiv.org/pdf/1405.4053v2.pdf>`_.\n",
      "    \n",
      "    The algorithms use either hierarchical softmax or negative sampling; see\n",
      "    `Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean: \"Efficient Estimation of Word Representations in\n",
      "    Vector Space, in Proceedings of Workshop at ICLR, 2013\" <https://arxiv.org/pdf/1301.3781.pdf>`_ and\n",
      "    `Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean: \"Distributed Representations of Words\n",
      "    and Phrases and their Compositionality. In Proceedings of NIPS, 2013\"\n",
      "    <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_.\n",
      "    \n",
      "    For a usage example, see the `Doc2vec tutorial\n",
      "    <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb>`_.\n",
      "    \n",
      "    **Make sure you have a C compiler before installing Gensim, to use the optimized doc2vec routines** (70x speedup\n",
      "    compared to plain NumPy implementation, https://rare-technologies.com/parallelizing-word2vec-in-python/).\n",
      "    \n",
      "    \n",
      "    Usage examples\n",
      "    ==============\n",
      "    \n",
      "    Initialize & train a model:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import common_texts\n",
      "        >>> from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
      "        >>>\n",
      "        >>> documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
      "        >>> model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
      "    \n",
      "    Persist a model to disk:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.test.utils import get_tmpfile\n",
      "        >>>\n",
      "        >>> fname = get_tmpfile(\"my_doc2vec_model\")\n",
      "        >>>\n",
      "        >>> model.save(fname)\n",
      "        >>> model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
      "    \n",
      "    If you're finished training a model (=no more updates, only querying, reduce memory usage), you can do:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
      "    \n",
      "    Infer vector for a new document:\n",
      "    \n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> vector = model.infer_vector([\"system\", \"response\"])\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        TaggedBrownCorpus\n",
      "        TaggedLineDocument\n",
      "    gensim.models.base_any2vec.BaseWordEmbeddingsModel(gensim.models.base_any2vec.BaseAny2VecModel)\n",
      "        Doc2Vec\n",
      "    Doctag(builtins.tuple)\n",
      "        Doctag\n",
      "    TaggedDocument(builtins.tuple)\n",
      "        TaggedDocument\n",
      "    gensim.models.word2vec.Word2VecTrainables(gensim.utils.SaveLoad)\n",
      "        Doc2VecTrainables\n",
      "    gensim.models.word2vec.Word2VecVocab(gensim.utils.SaveLoad)\n",
      "        Doc2VecVocab\n",
      "    \n",
      "    class Doc2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      "     |  Doc2Vec(documents=None, corpus_file=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)\n",
      "     |  \n",
      "     |  Class for training, using and evaluating neural networks described in\n",
      "     |  `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.\n",
      "     |  \n",
      "     |  Some important internal attributes are the following:\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      "     |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "     |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "     |  \n",
      "     |  docvecs : :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`\n",
      "     |      This object contains the paragraph vectors learned from the training data. There will be one such vector\n",
      "     |      for each unique document tag supplied during training. They may be individually accessed using the tag\n",
      "     |      as an indexed-access key. For example, if one of the training documents used a tag of 'doc003':\n",
      "     |  \n",
      "     |      .. sourcecode:: pycon\n",
      "     |  \n",
      "     |          >>> model.docvecs['doc003']\n",
      "     |  \n",
      "     |  vocabulary : :class:`~gensim.models.doc2vec.Doc2VecVocab`\n",
      "     |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      "     |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      "     |      sorting words by frequency, or discarding extremely rare words.\n",
      "     |  \n",
      "     |  trainables : :class:`~gensim.models.doc2vec.Doc2VecTrainables`\n",
      "     |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      "     |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      "     |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      "     |      The only addition to the underlying NN used in :class:`~gensim.models.word2vec.Word2Vec` is that the input\n",
      "     |      includes not only the word vectors of each word in the context, but also the paragraph vector.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Doc2Vec\n",
      "     |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      "     |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, tag)\n",
      "     |      Get the vector representation of (possible multi-term) tag.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      tag : {str, int, list of str, list of int}\n",
      "     |          The tag (or tags) to be looked up in the model.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      np.ndarray\n",
      "     |          The vector representations of each tag as a matrix (will be 1D if `tag` was a single tag)\n",
      "     |  \n",
      "     |  __init__(self, documents=None, corpus_file=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      "     |          Input corpus, can be simply a list of elements, but for larger corpora,consider an iterable that streams\n",
      "     |          the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\n",
      "     |          left uninitialized -- use if you plan to initialize it in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      "     |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      "     |          Documents' tags are assigned automatically and are equal to line number, as in\n",
      "     |          :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      "     |      dm : {1,0}, optional\n",
      "     |          Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
      "     |          Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
      "     |      vector_size : int, optional\n",
      "     |          Dimensionality of the feature vectors.\n",
      "     |      window : int, optional\n",
      "     |          The maximum distance between the current and predicted word within a sentence.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "     |      seed : int, optional\n",
      "     |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "     |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "     |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "     |          from OS thread scheduling.\n",
      "     |          In Python 3, reproducibility between interpreter launches also requires use of the `PYTHONHASHSEED`\n",
      "     |          environment variable to control hash randomization.\n",
      "     |      min_count : int, optional\n",
      "     |          Ignores all words with total frequency lower than this.\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Limits the RAM during vocabulary building; if there are more unique\n",
      "     |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "     |          Set to `None` for no limit.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      workers : int, optional\n",
      "     |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "     |      epochs : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      hs : {1,0}, optional\n",
      "     |          If 1, hierarchical softmax will be used for model training.\n",
      "     |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      "     |      negative : int, optional\n",
      "     |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "     |          should be drawn (usually between 5-20).\n",
      "     |          If set to 0, no negative sampling is used.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |      dm_mean : {1,0}, optional\n",
      "     |          If 0 , use the sum of the context word vectors. If 1, use the mean.\n",
      "     |          Only applies when `dm` is used in non-concatenative mode.\n",
      "     |      dm_concat : {1,0}, optional\n",
      "     |          If 1, use concatenation of context vectors rather than sum/average;\n",
      "     |          Note concatenation results in a much-larger model, as the input\n",
      "     |          is no longer the size of one (sampled or arithmetically combined) word vector, but the\n",
      "     |          size of the tag(s) and all words in the context strung together.\n",
      "     |      dm_tag_count : int, optional\n",
      "     |          Expected constant number of document tags per document, when using\n",
      "     |          dm_concat mode.\n",
      "     |      dbow_words : {1,0}, optional\n",
      "     |          If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\n",
      "     |          doc-vector training; If 0, only trains doc-vectors (faster).\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          List of callbacks that need to be executed/run at specific stages during training.\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Abbreviated name reflecting major configuration parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          Human readable representation of the models internal state.\n",
      "     |  \n",
      "     |  build_vocab(self, documents=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      "     |      Build vocabulary from a sequence of documents (can be a once-only generator stream).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      "     |          Can be simply a list of :class:`~gensim.models.doc2vec.TaggedDocument` elements, but for larger corpora,\n",
      "     |          consider an iterable that streams the documents directly from disk/network.\n",
      "     |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\n",
      "     |          and are equal to a line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      "     |      update : bool\n",
      "     |          If true, the new words in `documents` will be added to model's vocab.\n",
      "     |      progress_per : int\n",
      "     |          Indicates how many words to process before showing/updating the progress.\n",
      "     |      keep_raw_vocab : bool\n",
      "     |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Additional key word arguments passed to the internal vocabulary construction.\n",
      "     |  \n",
      "     |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      "     |      Build vocabulary from a dictionary of word frequencies.\n",
      "     |      \n",
      "     |      Build model vocabulary from a passed dictionary that contains a (word -> word count) mapping.\n",
      "     |      Words must be of type unicode strings.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      word_freq : dict of (str, int)\n",
      "     |          Word <-> count mapping.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      "     |      corpus_count : int, optional\n",
      "     |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during\n",
      "     |          :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      update : bool, optional\n",
      "     |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      "     |  \n",
      "     |  clear_sims(self)\n",
      "     |      Resets the current word vectors.\n",
      "     |  \n",
      "     |  delete_temporary_training_data(self, keep_doctags_vectors=True, keep_inference=True)\n",
      "     |      Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      keep_doctags_vectors : bool, optional\n",
      "     |          Set to False if you don't want to save doctags vectors. In this case you will not be able to use\n",
      "     |          :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar`,\n",
      "     |          :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors.similarity`, etc methods.\n",
      "     |      keep_inference : bool, optional\n",
      "     |          Set to False if you don't want to store parameters that are used for\n",
      "     |          :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector` method.\n",
      "     |  \n",
      "     |  estimate_memory(self, vocab_size=None, report=None)\n",
      "     |      Estimate required memory for a model using current settings.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vocab_size : int, optional\n",
      "     |          Number of raw words in the vocabulary.\n",
      "     |      report : dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the **specific** model's memory consuming members\n",
      "     |          to their size in bytes.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |          Includes members from the base classes as well as weights and tag lookup memory estimation specific to the\n",
      "     |          class.\n",
      "     |  \n",
      "     |  estimated_lookup_memory(self)\n",
      "     |      Get estimated memory for tag lookup, 0 if using pure int tags.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      int\n",
      "     |          The estimated RAM required to look up a tag in bytes.\n",
      "     |  \n",
      "     |  infer_vector(self, doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)\n",
      "     |      Infer a vector for given post-bulk training document.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Subsequent calls to this function may infer different representations for the same document.\n",
      "     |      For a more stable representation, increase the number of steps to assert a stricket convergence.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      doc_words : list of str\n",
      "     |          A document for which the vector representation will be inferred.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate. If unspecified, value from model initialization will be reused.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,\n",
      "     |          value from model initialization will be reused.\n",
      "     |      epochs : int, optional\n",
      "     |          Number of times to train the new document. Larger values take more time, but may improve\n",
      "     |          quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value\n",
      "     |          from model initialization will be reused.\n",
      "     |      steps : int, optional, deprecated\n",
      "     |          Previous name for `epochs`, still available for now for backward compatibility: if\n",
      "     |          `epochs` is unspecified but `steps` is, the `steps` value will be used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      np.ndarray\n",
      "     |          The inferred paragraph vector for the new document.\n",
      "     |  \n",
      "     |  init_sims(self, replace=False)\n",
      "     |      Pre-compute L2-normalized vectors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      replace : bool\n",
      "     |          If True - forget the original vectors and only keep the normalized ones to saved RAM (also you can't\n",
      "     |          continue training if call it with `replace=True`).\n",
      "     |  \n",
      "     |  reset_from(self, other_model)\n",
      "     |      Copy shareable data structures from another (possibly pre-trained) model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other_model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "     |          Other model whose internal data structures will be copied over to the current object.\n",
      "     |  \n",
      "     |  save_word2vec_format(self, fname, doctag_vec=False, word_vec=True, prefix='*dt_', fvocab=None, binary=False)\n",
      "     |      Store the input-hidden weight matrix in the same format used by the original C word2vec-tool.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          The file path used to save the vectors in.\n",
      "     |      doctag_vec : bool, optional\n",
      "     |          Indicates whether to store document vectors.\n",
      "     |      word_vec : bool, optional\n",
      "     |          Indicates whether to store word vectors.\n",
      "     |      prefix : str, optional\n",
      "     |          Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag\n",
      "     |          and word vocab.\n",
      "     |      fvocab : str, optional\n",
      "     |          Optional file path used to save the vocabulary.\n",
      "     |      binary : bool, optional\n",
      "     |          If True, the data will be saved in binary word2vec format, otherwise - will be saved in plain text.\n",
      "     |  \n",
      "     |  train(self, documents=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=())\n",
      "     |      Update the model's neural weights.\n",
      "     |      \n",
      "     |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "     |      progress-percentage logging, either `total_examples` (count of documents) or `total_words` (count of\n",
      "     |      raw words in documents) **MUST** be provided. If `documents` is the same corpus\n",
      "     |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "     |      you can simply use `total_examples=self.corpus_count`.\n",
      "     |      \n",
      "     |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "     |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "     |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once,\n",
      "     |      you can set `epochs=self.iter`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      "     |          Can be simply a list of elements, but for larger corpora,consider an iterable that streams\n",
      "     |          the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\n",
      "     |          left uninitialized -- use if you plan to initialize it in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\n",
      "     |          and are equal to line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      "     |      total_examples : int, optional\n",
      "     |          Count of documents.\n",
      "     |      total_words : int, optional\n",
      "     |          Count of raw words in documents.\n",
      "     |      epochs : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      start_alpha : float, optional\n",
      "     |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "     |          for this one call to `train`.\n",
      "     |          Use only if making multiple calls to `train`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      end_alpha : float, optional\n",
      "     |          Final learning rate. Drops linearly from `start_alpha`.\n",
      "     |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to\n",
      "     |          :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\n",
      "     |          Use only if making multiple calls to :meth:`~gensim.models.doc2vec.Doc2Vec.train`, when you want to manage\n",
      "     |          the alpha learning-rate yourself (not recommended).\n",
      "     |      word_count : int, optional\n",
      "     |          Count of words already trained. Set this to 0 for the usual\n",
      "     |          case of training on all words in documents.\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          List of callbacks that need to be executed/run at specific stages during training.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  load(*args, **kwargs) from builtins.type\n",
      "     |      Load a previously saved :class:`~gensim.models.doc2vec.Doc2Vec` model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the saved file.\n",
      "     |      *args : object\n",
      "     |          Additional arguments, see `~gensim.models.base_any2vec.BaseWordEmbeddingsModel.load`.\n",
      "     |      **kwargs : object\n",
      "     |          Additional arguments, see `~gensim.models.base_any2vec.BaseWordEmbeddingsModel.load`.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.doc2vec.Doc2Vec.save`\n",
      "     |          Save :class:`~gensim.models.doc2vec.Doc2Vec` model.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "     |          Loaded model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  dbow\n",
      "     |      Indicates whether 'distributed bag of words' (PV-DBOW) will be used, else 'distributed memory'\n",
      "     |      (PV-DM) is used.\n",
      "     |  \n",
      "     |  dm\n",
      "     |      Indicates whether 'distributed memory' (PV-DM) will be used, else 'distributed bag of words'\n",
      "     |      (PV-DBOW) is used.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  doesnt_match(self, words)\n",
      "     |      Deprecated, use self.wv.doesnt_match() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      "     |  \n",
      "     |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      "     |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      "     |  \n",
      "     |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      "     |      Deprecated, use self.wv.most_similar() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      "     |  \n",
      "     |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      "     |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for\n",
      "     |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      "     |  \n",
      "     |  n_similarity(self, ws1, ws2)\n",
      "     |      Deprecated, use self.wv.n_similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      "     |  \n",
      "     |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      "     |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      "     |  \n",
      "     |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      "     |      Deprecated, use self.wv.similar_by_word() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      "     |  \n",
      "     |  similarity(self, w1, w2)\n",
      "     |      Deprecated, use self.wv.similarity() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      "     |  \n",
      "     |  wmdistance(self, document1, document2)\n",
      "     |      Deprecated, use self.wv.wmdistance() instead.\n",
      "     |      \n",
      "     |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      "     |  \n",
      "     |  cum_table\n",
      "     |  \n",
      "     |  hashfxn\n",
      "     |  \n",
      "     |  iter\n",
      "     |  \n",
      "     |  layer1_size\n",
      "     |  \n",
      "     |  min_count\n",
      "     |  \n",
      "     |  sample\n",
      "     |  \n",
      "     |  syn0_lockf\n",
      "     |  \n",
      "     |  syn1\n",
      "     |  \n",
      "     |  syn1neg\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.base_any2vec.BaseAny2VecModel:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, **kwargs)\n",
      "     |      \"Save the object to file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : {str, file-like object}\n",
      "     |          Path to file where the model will be persisted.\n",
      "     |      **kwargs : object\n",
      "     |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.base_any2vec.BaseAny2VecModel.load`\n",
      "     |          Method for load model after current method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Doc2VecTrainables(gensim.models.word2vec.Word2VecTrainables)\n",
      "     |  Doc2VecTrainables(dm=1, dm_concat=0, dm_tag_count=1, vector_size=100, seed=1, hashfxn=<built-in function hash>, window=5)\n",
      "     |  \n",
      "     |  Represents the inner shallow neural network used to train :class:`~gensim.models.doc2vec.Doc2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Doc2VecTrainables\n",
      "     |      gensim.models.word2vec.Word2VecTrainables\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dm=1, dm_concat=0, dm_tag_count=1, vector_size=100, seed=1, hashfxn=<built-in function hash>, window=5)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_doctag_trainables(self, doc_words, vector_size)\n",
      "     |  \n",
      "     |  prepare_weights(self, hs, negative, wv, docvecs, update=False)\n",
      "     |      Build tables and model weights based on final vocabulary settings.\n",
      "     |  \n",
      "     |  reset_doc_weights(self, docvecs)\n",
      "     |  \n",
      "     |  reset_weights(self, hs, negative, wv, docvecs, vocabulary=None)\n",
      "     |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.word2vec.Word2VecTrainables:\n",
      "     |  \n",
      "     |  seeded_vector(self, seed_string, vector_size)\n",
      "     |      Get a random vector (but deterministic by seed_string).\n",
      "     |  \n",
      "     |  update_weights(self, hs, negative, wv)\n",
      "     |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gensim.models.word2vec.Word2VecTrainables:\n",
      "     |  \n",
      "     |  __slotnames__ = []\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Doc2VecVocab(gensim.models.word2vec.Word2VecVocab)\n",
      "     |  Doc2VecVocab(max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, ns_exponent=0.75)\n",
      "     |  \n",
      "     |  Vocabulary used by :class:`~gensim.models.doc2vec.Doc2Vec`.\n",
      "     |  \n",
      "     |  This includes a mapping from words found in the corpus to their total frequency count.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Doc2VecVocab\n",
      "     |      gensim.models.word2vec.Word2VecVocab\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_vocab_size=None, min_count=5, sample=0.001, sorted_vocab=True, null_word=0, ns_exponent=0.75)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Maximum number of words in the Vocabulary. Used to limit the RAM during vocabulary building;\n",
      "     |          if there are more unique words than this, then prune the infrequent ones.\n",
      "     |          Every 10 million word types need about 1GB of RAM, set to `None` for no limit.\n",
      "     |      min_count : int\n",
      "     |          Words with frequency lower than this limit will be discarded from the vocabulary.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      sorted_vocab : bool\n",
      "     |          If True, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "     |      null_word : {0, 1}\n",
      "     |          If True, a null pseudo-word will be created for padding when using concatenative L1 (run-of-words).\n",
      "     |          This word is only ever input – never predicted – so count, huffman-point, etc doesn't matter.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |  \n",
      "     |  indexed_doctags(self, doctag_tokens, docvecs)\n",
      "     |      Get the indexes and backing-arrays used in training examples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      doctag_tokens : list of {str, int}\n",
      "     |          A list of tags for which we want the index.\n",
      "     |      docvecs : list of :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`\n",
      "     |          Vector representations of the documents in the corpus. Each vector has size == `vector_size`\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of int\n",
      "     |          Indices of the provided tag keys.\n",
      "     |  \n",
      "     |  scan_vocab(self, documents=None, corpus_file=None, docvecs=None, progress_per=10000, trim_rule=None)\n",
      "     |      Create the models Vocabulary: A mapping from unique words in the corpus to their frequency count.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      documents : iterable of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      "     |          The tagged documents used to create the vocabulary. Their tags can be either str tokens or ints (faster).\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      docvecs : list of :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`\n",
      "     |          The vector representations of the documents in our corpus. Each of them has a size == `vector_size`.\n",
      "     |      progress_per : int\n",
      "     |          Progress will be logged every `progress_per` documents.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during\n",
      "     |          :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\n",
      "     |      \n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      (int, int)\n",
      "     |          Tuple of (Total words in the corpus, number of documents)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.models.word2vec.Word2VecVocab:\n",
      "     |  \n",
      "     |  add_null_word(self, wv)\n",
      "     |  \n",
      "     |  create_binary_tree(self, wv)\n",
      "     |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      "     |      word counts. Frequent words will have shorter binary codes.\n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  make_cum_table(self, wv, domain=2147483647)\n",
      "     |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      "     |      drawing random words in the negative-sampling training routines.\n",
      "     |      \n",
      "     |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      "     |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      "     |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      "     |      \n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |  \n",
      "     |  prepare_vocab(self, hs, negative, wv, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      "     |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      "     |      and `sample` (controlling the downsampling of more-frequent words).\n",
      "     |      \n",
      "     |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      "     |      report the size of the retained vocabulary, effective corpus length, and\n",
      "     |      estimated memory requirements. Results are both printed via logging and\n",
      "     |      returned as a dict.\n",
      "     |      \n",
      "     |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      "     |      unless `keep_raw_vocab` is set.\n",
      "     |  \n",
      "     |  sort_vocab(self, wv)\n",
      "     |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from gensim.models.word2vec.Word2VecVocab:\n",
      "     |  \n",
      "     |  __slotnames__ = []\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |      \n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Doctag(Doctag)\n",
      "     |  Doctag(offset, word_count, doc_count)\n",
      "     |  \n",
      "     |  A string document tag discovered during the initial vocabulary scan.\n",
      "     |  The document-vector equivalent of a Vocab object.\n",
      "     |  \n",
      "     |  Will not be used if all presented document tags are ints.\n",
      "     |  \n",
      "     |  The offset is only the true index into the `doctags_syn0`/`doctags_syn0_lockf`\n",
      "     |  if-and-only-if no raw-int tags were used.\n",
      "     |  If any raw-int tags were used, string :class:`~gensim.models.doc2vec.Doctag` vectors begin at index\n",
      "     |  `(max_rawint + 1)`, so the true index is `(rawint_index + 1 + offset)`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors._index_to_doctag`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Doctag\n",
      "     |      Doctag\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  repeat(self, word_count)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Doctag:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new OrderedDict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(_self, **kwds)\n",
      "     |      Return a new Doctag object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Doctag:\n",
      "     |  \n",
      "     |  _make(iterable) from builtins.type\n",
      "     |      Make a new Doctag object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Doctag:\n",
      "     |  \n",
      "     |  __new__(_cls, offset, word_count, doc_count)\n",
      "     |      Create new instance of Doctag(offset, word_count, doc_count)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Doctag:\n",
      "     |  \n",
      "     |  offset\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  word_count\n",
      "     |      Alias for field number 1\n",
      "     |  \n",
      "     |  doc_count\n",
      "     |      Alias for field number 2\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Doctag:\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('offset', 'word_count', 'doc_count')\n",
      "     |  \n",
      "     |  _fields_defaults = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "    \n",
      "    class TaggedBrownCorpus(builtins.object)\n",
      "     |  TaggedBrownCorpus(dirname)\n",
      "     |  \n",
      "     |  Reader for the `Brown corpus (part of NLTK data) <http://www.nltk.org/book/ch02.html#tab-brown-sources>`_.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dirname)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dirname : str\n",
      "     |          Path to folder with Brown corpus.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the corpus.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      :class:`~gensim.models.doc2vec.TaggedDocument`\n",
      "     |          Document from `source`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TaggedDocument(TaggedDocument)\n",
      "     |  TaggedDocument(words, tags)\n",
      "     |  \n",
      "     |  Represents a document along with a tag, input document format for :class:`~gensim.models.doc2vec.Doc2Vec`.\n",
      "     |  \n",
      "     |  A single document, made up of `words` (a list of unicode string tokens) and `tags` (a list of tokens).\n",
      "     |  Tags may be one or more unicode string tokens, but typical practice (which will also be the most memory-efficient)\n",
      "     |  is for the tags list to include a unique integer id as the only tag.\n",
      "     |  \n",
      "     |  Replaces \"sentence as a list of words\" from :class:`gensim.models.word2vec.Word2Vec`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TaggedDocument\n",
      "     |      TaggedDocument\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Human readable representation of the object's state, used for debugging.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |         Human readable representation of the object's state (words and tags).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TaggedDocument:\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a nicely formatted representation string\n",
      "     |  \n",
      "     |  _asdict(self)\n",
      "     |      Return a new OrderedDict which maps field names to their values.\n",
      "     |  \n",
      "     |  _replace(_self, **kwds)\n",
      "     |      Return a new TaggedDocument object replacing specified fields with new values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from TaggedDocument:\n",
      "     |  \n",
      "     |  _make(iterable) from builtins.type\n",
      "     |      Make a new TaggedDocument object from a sequence or iterable\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from TaggedDocument:\n",
      "     |  \n",
      "     |  __new__(_cls, words, tags)\n",
      "     |      Create new instance of TaggedDocument(words, tags)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from TaggedDocument:\n",
      "     |  \n",
      "     |  words\n",
      "     |      Alias for field number 0\n",
      "     |  \n",
      "     |  tags\n",
      "     |      Alias for field number 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from TaggedDocument:\n",
      "     |  \n",
      "     |  _field_defaults = {}\n",
      "     |  \n",
      "     |  _fields = ('words', 'tags')\n",
      "     |  \n",
      "     |  _fields_defaults = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "    \n",
      "    class TaggedLineDocument(builtins.object)\n",
      "     |  TaggedLineDocument(source)\n",
      "     |  \n",
      "     |  Iterate over a file that contains documents: one line = :class:`~gensim.models.doc2vec.TaggedDocument` object.\n",
      "     |  \n",
      "     |  Words are expected to be already preprocessed and separated by whitespace. Document tags are constructed\n",
      "     |  automatically from the document line number (each document gets a unique integer tag).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, source)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : string or a file-like object\n",
      "     |          Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |      \n",
      "     |          >>> from gensim.test.utils import datapath\n",
      "     |          >>> from gensim.models.doc2vec import TaggedLineDocument\n",
      "     |          >>>\n",
      "     |          >>> for document in TaggedLineDocument(datapath(\"head500.noblanks.cor\")):\n",
      "     |          ...     pass\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the lines in the source.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      :class:`~gensim.models.doc2vec.TaggedDocument`\n",
      "     |          Document from `source` specified in the constructor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    LabeledSentence(words, tags)\n",
      "        Deprecated, use :class:`~gensim.models.doc2vec.TaggedDocument` instead.\n",
      "    \n",
      "    d2v_train_epoch_dbow(...)\n",
      "        d2v_train_epoch_dbow(model, corpus_file, offset, start_doctag, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, work, neu1, docvecs_count, word_vectors=None, word_locks=None, train_words=False, learn_doctags=True, learn_words=True, learn_hidden=True, doctag_vectors=None, doctag_locks=None)\n",
      "        Train distributed bag of words model (\"PV-DBOW\") by training on a corpus file.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "                The FastText model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            train_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both** `learn_words`\n",
      "                and `train_words` are set to True.\n",
      "            learn_doctags : bool, optional\n",
      "                Whether the tag vectors should be updated.\n",
      "            learn_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\n",
      "                `learn_words` and `train_words` are set to True.\n",
      "            learn_hidden : bool, optional\n",
      "                Whether or not the weights of the hidden layer will be updated.\n",
      "            word_vectors : numpy.ndarray, optional\n",
      "                The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.\n",
      "            word_locks : numpy.ndarray, optional\n",
      "                A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,\n",
      "                a value of 1 allows to update word-vectors.\n",
      "            doctag_vectors : numpy.ndarray, optional\n",
      "                Vector representations of the tags. If None, these will be retrieved from the model.\n",
      "            doctag_locks : numpy.ndarray, optional\n",
      "                The lock factors for each tag, same as `word_locks`, but for document-vectors.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the input document that were actually used for training.\n",
      "    \n",
      "    d2v_train_epoch_dm(...)\n",
      "        d2v_train_epoch_dm(model, corpus_file, offset, start_doctag, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, work, neu1, docvecs_count, word_vectors=None, word_locks=None, learn_doctags=True, learn_words=True, learn_hidden=True, doctag_vectors=None, doctag_locks=None)\n",
      "        Train distributed memory model (\"PV-DM\") by training on a corpus file.\n",
      "            This method implements the DM model with a projection (input) layer that is either the sum or mean of the context\n",
      "            vectors, depending on the model's `dm_mean` configuration field.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "                The FastText model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            learn_doctags : bool, optional\n",
      "                Whether the tag vectors should be updated.\n",
      "            learn_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\n",
      "                `learn_words` and `train_words` are set to True.\n",
      "            learn_hidden : bool, optional\n",
      "                Whether or not the weights of the hidden layer will be updated.\n",
      "            word_vectors : numpy.ndarray, optional\n",
      "                The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.\n",
      "            word_locks : numpy.ndarray, optional\n",
      "                A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,\n",
      "                a value of 1 allows to update word-vectors.\n",
      "            doctag_vectors : numpy.ndarray, optional\n",
      "                Vector representations of the tags. If None, these will be retrieved from the model.\n",
      "            doctag_locks : numpy.ndarray, optional\n",
      "                The lock factors for each tag, same as `word_locks`, but for document-vectors.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the input document that were actually used for training.\n",
      "    \n",
      "    d2v_train_epoch_dm_concat(...)\n",
      "        d2v_train_epoch_dm_concat(model, corpus_file, offset, start_doctag, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, work, neu1, docvecs_count, word_vectors=None, word_locks=None, learn_doctags=True, learn_words=True, learn_hidden=True, doctag_vectors=None, doctag_locks=None)\n",
      "        Train distributed memory model (\"PV-DM\") by training on a corpus file, using a concatenation of the context\n",
      "             window word vectors (rather than a sum or average).\n",
      "             This might be slower since the input at each batch will be significantly larger.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "                The FastText model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            learn_doctags : bool, optional\n",
      "                Whether the tag vectors should be updated.\n",
      "            learn_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\n",
      "                `learn_words` and `train_words` are set to True.\n",
      "            learn_hidden : bool, optional\n",
      "                Whether or not the weights of the hidden layer will be updated.\n",
      "            word_vectors : numpy.ndarray, optional\n",
      "                The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.\n",
      "            word_locks : numpy.ndarray, optional\n",
      "                A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,\n",
      "                a value of 1 allows to update word-vectors.\n",
      "            doctag_vectors : numpy.ndarray, optional\n",
      "                Vector representations of the tags. If None, these will be retrieved from the model.\n",
      "            doctag_locks : numpy.ndarray, optional\n",
      "                The lock factors for each tag, same as `word_locks`, but for document-vectors.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the input document that were actually used for training.\n",
      "    \n",
      "    default_timer = perf_counter(...)\n",
      "        perf_counter() -> float\n",
      "        \n",
      "        Performance counter for benchmarking.\n",
      "    \n",
      "    empty(...)\n",
      "        empty(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, without initializing entries.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of int\n",
      "            Shape of the empty array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            Desired output data-type for the array, e.g, `numpy.int8`. Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of uninitialized (arbitrary) data of the given shape, dtype, and\n",
      "            order.  Object arrays will be initialized to None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        empty_like : Return an empty array with shape and type of input.\n",
      "        ones : Return a new array setting values to one.\n",
      "        zeros : Return a new array setting values to zero.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `empty`, unlike `zeros`, does not set the array values to zero,\n",
      "        and may therefore be marginally faster.  On the other hand, it requires\n",
      "        the user to manually set all the values in the array, and should be\n",
      "        used with caution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.empty([2, 2])\n",
      "        array([[ -9.74499359e+001,   6.69583040e-309],\n",
      "               [  2.13182611e-314,   3.06959433e-309]])         #uninitialized\n",
      "        \n",
      "        >>> np.empty([2, 2], dtype=int)\n",
      "        array([[-1073741821, -1067949133],\n",
      "               [  496041986,    19249760]])                     #uninitialized\n",
      "    \n",
      "    train_batch_sg(...)\n",
      "        train_batch_sg(model, sentences, alpha, _work, compute_loss)\n",
      "        Update skip-gram model by training on a batch of sentences.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "    \n",
      "    train_document_dbow(...)\n",
      "        train_document_dbow(model, doc_words, doctag_indexes, alpha, work=None, train_words=False, learn_doctags=True, learn_words=True, learn_hidden=True, word_vectors=None, word_locks=None, doctag_vectors=None, doctag_locks=None)\n",
      "        Update distributed bag of words model (\"PV-DBOW\") by training on a single document.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train` and\n",
      "            :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "                The model to train.\n",
      "            doc_words : list of str\n",
      "                The input document as a list of words to be used for training. Each word will be looked up in\n",
      "                the model's vocabulary.\n",
      "            doctag_indexes : list of int\n",
      "                Indices into `doctag_vectors` used to obtain the tags of the document.\n",
      "            alpha : float\n",
      "                Learning rate.\n",
      "            work : list of float, optional\n",
      "                Updates to be performed on each neuron in the hidden layer of the underlying network.\n",
      "            train_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both** `learn_words`\n",
      "                and `train_words` are set to True.\n",
      "            learn_doctags : bool, optional\n",
      "                Whether the tag vectors should be updated.\n",
      "            learn_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\n",
      "                `learn_words` and `train_words` are set to True.\n",
      "            learn_hidden : bool, optional\n",
      "                Whether or not the weights of the hidden layer will be updated.\n",
      "            word_vectors : numpy.ndarray, optional\n",
      "                The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.\n",
      "            word_locks : numpy.ndarray, optional\n",
      "                A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,\n",
      "                a value of 1 allows to update word-vectors.\n",
      "            doctag_vectors : numpy.ndarray, optional\n",
      "                Vector representations of the tags. If None, these will be retrieved from the model.\n",
      "            doctag_locks : numpy.ndarray, optional\n",
      "                The lock factors for each tag, same as `word_locks`, but for document-vectors.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the input document that were actually used for training.\n",
      "    \n",
      "    train_document_dm(...)\n",
      "        train_document_dm(model, doc_words, doctag_indexes, alpha, work=None, neu1=None, learn_doctags=True, learn_words=True, learn_hidden=True, word_vectors=None, word_locks=None, doctag_vectors=None, doctag_locks=None)\n",
      "        Update distributed memory model (\"PV-DM\") by training on a single document.\n",
      "            This method implements the DM model with a projection (input) layer that is either the sum or mean of the context\n",
      "            vectors, depending on the model's `dm_mean` configuration field.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train` and\n",
      "            :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "                The model to train.\n",
      "            doc_words : list of str\n",
      "                The input document as a list of words to be used for training. Each word will be looked up in\n",
      "                the model's vocabulary.\n",
      "            doctag_indexes : list of int\n",
      "                Indices into `doctag_vectors` used to obtain the tags of the document.\n",
      "            alpha : float\n",
      "                Learning rate.\n",
      "            work : np.ndarray, optional\n",
      "                Private working memory for each worker.\n",
      "            neu1 : np.ndarray, optional\n",
      "                Private working memory for each worker.\n",
      "            learn_doctags : bool, optional\n",
      "                Whether the tag vectors should be updated.\n",
      "            learn_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\n",
      "                `learn_words` and `train_words` are set to True.\n",
      "            learn_hidden : bool, optional\n",
      "                Whether or not the weights of the hidden layer will be updated.\n",
      "            word_vectors : numpy.ndarray, optional\n",
      "                The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.\n",
      "            word_locks : numpy.ndarray, optional\n",
      "                A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,\n",
      "                a value of 1 allows to update word-vectors.\n",
      "            doctag_vectors : numpy.ndarray, optional\n",
      "                Vector representations of the tags. If None, these will be retrieved from the model.\n",
      "            doctag_locks : numpy.ndarray, optional\n",
      "                The lock factors for each tag, same as `word_locks`, but for document-vectors.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the input document that were actually used for training.\n",
      "    \n",
      "    train_document_dm_concat(...)\n",
      "        train_document_dm_concat(model, doc_words, doctag_indexes, alpha, work=None, neu1=None, learn_doctags=True, learn_words=True, learn_hidden=True, word_vectors=None, word_locks=None, doctag_vectors=None, doctag_locks=None)\n",
      "        Update distributed memory model (\"PV-DM\") by training on a single document, using a concatenation of the context\n",
      "             window word vectors (rather than a sum or average).\n",
      "             This might be slower since the input at each batch will be significantly larger.\n",
      "        \n",
      "            Called internally from :meth:`~gensim.models.doc2vec.Doc2Vec.train` and\n",
      "            :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector`.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      "                The model to train.\n",
      "            doc_words : list of str\n",
      "                The input document as a list of words to be used for training. Each word will be looked up in\n",
      "                the model's vocabulary.\n",
      "            doctag_indexes : list of int\n",
      "                Indices into `doctag_vectors` used to obtain the tags of the document.\n",
      "            alpha : float, optional\n",
      "                Learning rate.\n",
      "            work : np.ndarray, optional\n",
      "                Private working memory for each worker.\n",
      "            neu1 : np.ndarray, optional\n",
      "                Private working memory for each worker.\n",
      "            learn_doctags : bool, optional\n",
      "                Whether the tag vectors should be updated.\n",
      "            learn_words : bool, optional\n",
      "                Word vectors will be updated exactly as per Word2Vec skip-gram training only if **both**\n",
      "                `learn_words` and `train_words` are set to True.\n",
      "            learn_hidden : bool, optional\n",
      "                Whether or not the weights of the hidden layer will be updated.\n",
      "            word_vectors : numpy.ndarray, optional\n",
      "                The vector representation for each word in the vocabulary. If None, these will be retrieved from the model.\n",
      "            word_locks : numpy.ndarray, optional\n",
      "                A learning lock factor for each weight in the hidden layer for words, value 0 completely blocks updates,\n",
      "                a value of 1 allows to update word-vectors.\n",
      "            doctag_vectors : numpy.ndarray, optional\n",
      "                Vector representations of the tags. If None, these will be retrieved from the model.\n",
      "            doctag_locks : numpy.ndarray, optional\n",
      "                The lock factors for each tag, same as `word_locks`, but for document-vectors.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the input document that were actually used for training.\n",
      "    \n",
      "    zeros(...)\n",
      "        zeros(shape, dtype=float, order='C')\n",
      "        \n",
      "        Return a new array of given shape and type, filled with zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : int or tuple of ints\n",
      "            Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n",
      "        dtype : data-type, optional\n",
      "            The desired data-type for the array, e.g., `numpy.int8`.  Default is\n",
      "            `numpy.float64`.\n",
      "        order : {'C', 'F'}, optional, default: 'C'\n",
      "            Whether to store multi-dimensional data in row-major\n",
      "            (C-style) or column-major (Fortran-style) order in\n",
      "            memory.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Array of zeros with the given shape, dtype, and order.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zeros_like : Return an array of zeros with shape and type of input.\n",
      "        empty : Return a new uninitialized array.\n",
      "        ones : Return a new array setting values to one.\n",
      "        full : Return a new array of given shape filled with value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.zeros(5)\n",
      "        array([ 0.,  0.,  0.,  0.,  0.])\n",
      "        \n",
      "        >>> np.zeros((5,), dtype=int)\n",
      "        array([0, 0, 0, 0, 0])\n",
      "        \n",
      "        >>> np.zeros((2, 1))\n",
      "        array([[ 0.],\n",
      "               [ 0.]])\n",
      "        \n",
      "        >>> s = (2,2)\n",
      "        >>> np.zeros(s)\n",
      "        array([[ 0.,  0.],\n",
      "               [ 0.,  0.]])\n",
      "        \n",
      "        >>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n",
      "        array([(0, 0), (0, 0)],\n",
      "              dtype=[('x', '<i4'), ('y', '<i4')])\n",
      "\n",
      "DATA\n",
      "    CORPUSFILE_VERSION = 1\n",
      "    FAST_VERSION = 0\n",
      "    __warningregistry__ = {'version': 20}\n",
      "    integer_types = (<class 'int'>,)\n",
      "    logger = <Logger gensim.models.doc2vec (WARNING)>\n",
      "    np_add = <ufunc 'add'>\n",
      "    string_types = (<class 'str'>,)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\bellachenhui\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\doc2vec.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=sentences[i], tags=[str(i)]) for i, _d in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'], tags=['0']),\n",
       " TaggedDocument(words=['this', 'is', 'the', 'second', 'sentence'], tags=['1']),\n",
       " TaggedDocument(words=['yet', 'another', 'sentence'], tags=['2']),\n",
       " TaggedDocument(words=['one', 'more', 'sentence'], tags=['3']),\n",
       " TaggedDocument(words=['and', 'the', 'final', 'sentence'], tags=['4'])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BellaChenhui\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\Users\\BellaChenhui\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20 # Please change this to a bigger number e.g. 100, 200 for DementiaBank data\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BellaChenhui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [-0.01602854 -0.0231523   0.0136775   0.00577613  0.00304549 -0.02422594\n",
      " -0.02425691 -0.00538873 -0.00061412 -0.01914475  0.0235144   0.01585912\n",
      "  0.01045371  0.00595883  0.00361456 -0.00014963  0.01134022  0.00721041\n",
      "  0.01600407 -0.02234366]\n",
      "[('0', 0.9924258589744568), ('4', 0.9906629920005798), ('3', 0.9867048263549805), ('2', 0.9826027154922485)]\n",
      "[ 0.07113843  0.09749075 -0.1132413   0.45199177 -0.00913651  0.00598469\n",
      " -0.13697578  0.12623683 -0.1973811  -0.14839289  0.25066903 -0.31101796\n",
      " -0.01864866  0.01188668  0.11148507  0.10085799  0.42241964 -0.24753405\n",
      "  0.07853296  0.04749978]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#  You don't need the following part\n",
    "# ----------------------------------------------------------------\n",
    "# #to find the vector of a document which is not in training data\n",
    "# test_data = word_tokenize(\"yes another sentence\".lower())\n",
    "# v1 = model.infer_vector(test_data)\n",
    "# print(\"V1_infer\", v1)\n",
    "\n",
    "# # to find most similar doc using tags\n",
    "# similar_doc = model.docvecs.most_similar('1')\n",
    "# print(similar_doc)\n",
    "\n",
    "# # to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "# print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07679146  0.28624466 -0.16733675  0.8488375  -0.07094532  0.02924621\n",
      " -0.30556533  0.18786085 -0.45458636 -0.2405564   0.49366015 -0.66271514\n",
      " -0.01870482  0.07587192  0.24987122  0.262275    0.858823   -0.4847591\n",
      "  0.11114644  0.07649717]\n",
      "[ 0.07113843  0.09749075 -0.1132413   0.45199177 -0.00913651  0.00598469\n",
      " -0.13697578  0.12623683 -0.1973811  -0.14839289  0.25066903 -0.31101796\n",
      " -0.01864866  0.01188668  0.11148507  0.10085799  0.42241964 -0.24753405\n",
      "  0.07853296  0.04749978]\n",
      "[ 0.01965942  0.10954224 -0.08608779  0.3978017   0.00452937 -0.03310821\n",
      " -0.13553347  0.05730376 -0.1774053  -0.11355928  0.16917507 -0.29533562\n",
      " -0.04180315  0.07855614  0.08579835  0.09408797  0.40046978 -0.22524883\n",
      "  0.01128695  0.05837302]\n"
     ]
    }
   ],
   "source": [
    "# embedding of the first sentence\n",
    "print(model.docvecs[0])\n",
    "\n",
    "# embedding of the second sentence\n",
    "print(model.docvecs[1])\n",
    "\n",
    "# embedding of the third sentence\n",
    "print(model.docvecs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07679146,  0.28624466, -0.16733675,  0.8488375 , -0.07094532,\n",
       "         0.02924621, -0.30556533,  0.18786085, -0.45458636, -0.2405564 ,\n",
       "         0.49366015, -0.66271514, -0.01870482,  0.07587192,  0.24987122,\n",
       "         0.262275  ,  0.858823  , -0.4847591 ,  0.11114644,  0.07649717],\n",
       "       [ 0.07113843,  0.09749075, -0.1132413 ,  0.45199177, -0.00913651,\n",
       "         0.00598469, -0.13697578,  0.12623683, -0.1973811 , -0.14839289,\n",
       "         0.25066903, -0.31101796, -0.01864866,  0.01188668,  0.11148507,\n",
       "         0.10085799,  0.42241964, -0.24753405,  0.07853296,  0.04749978],\n",
       "       [ 0.01965942,  0.10954224, -0.08608779,  0.3978017 ,  0.00452937,\n",
       "        -0.03310821, -0.13553347,  0.05730376, -0.1774053 , -0.11355928,\n",
       "         0.16917507, -0.29533562, -0.04180315,  0.07855614,  0.08579835,\n",
       "         0.09408797,  0.40046978, -0.22524883,  0.01128695,  0.05837302],\n",
       "       [ 0.04604028,  0.1433122 , -0.08084972,  0.43864402, -0.02897015,\n",
       "        -0.03804348, -0.13737158,  0.12846795, -0.21641205, -0.16643226,\n",
       "         0.24145809, -0.35225335, -0.05965697,  0.06717003,  0.07886524,\n",
       "         0.15568702,  0.4125063 , -0.20488885,  0.05407526,  0.06664953],\n",
       "       [ 0.02914321,  0.12596002, -0.07611924,  0.4234699 , -0.0275925 ,\n",
       "         0.00608143, -0.106506  ,  0.1301805 , -0.21687269, -0.10915698,\n",
       "         0.2247852 , -0.3226866 , -0.01683904,  0.03936456,  0.08616526,\n",
       "         0.15147686,  0.4316323 , -0.22550552,  0.05016934,  0.06090511]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the vectors to numpy array for training logistic regression model\n",
    "import numpy as np\n",
    "X = [list(model.docvecs[i]) for i in range(len(sentences))]\n",
    "X = np.asarray(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit586c4fbfaa78491189fcaa770ac93e25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
